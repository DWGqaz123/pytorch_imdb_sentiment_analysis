{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于PyTorch的transformer情感分析项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.7.0\n",
      "Transformers: 4.40.0\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------\n",
    "### 阶段一：导入库和工具函数\n",
    "### -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from collections import Counter # 不再需要\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os # 用于文件路径操作\n",
    "\n",
    "# Hugging Face Transformers 库\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# 导入之前的通用工具函数\n",
    "import sys\n",
    "# 假设 Notebook 路径是 pytorch_imdb_sentiment_analysis/notebooks/\n",
    "# 所以回到项目根目录是 '..'\n",
    "sys.path.append('..') \n",
    "from utils.train_eval_utils import set_seed\n",
    "# 不需要直接导入 train_model, evaluate_model, save_model, load_model，因为我们将在Notebook中直接编写训练评估循环。\n",
    "# 这样更灵活，避免 train_eval_utils.py 依赖HuggingFace的特定输出格式。\n",
    "\n",
    "# 导入文本预处理工具\n",
    "from utils.text_preprocessing import load_imdb_data, clean_text # 导入这两个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device for training.\n",
      "Random seed set to 66\n"
     ]
    }
   ],
   "source": [
    "# 2. 设备设置\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not available, using CPU for training.\")\n",
    "\n",
    "# 3. 设置随机种子\n",
    "SEED = 66\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------\n",
    "### 阶段二：构建数据集和加载数据\n",
    "### -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading real IMDB training data...\n",
      "Cleaning training data (this may take a moment)...\n",
      "Loading real IMDB testing data...\n",
      "Cleaning testing data (this may take a moment)...\n",
      "Loaded 25000 training samples.\n",
      "Loaded 25000 testing samples.\n",
      "Example cleaned positive review (first 100 chars): for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem imagin...\n",
      "Example cleaned negative review (first 100 chars): working with one of the best shakespeare sources this film manages to be creditable to its source wh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pytorch_env_1/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 25000 training dataset samples.\n",
      "Created 25000 testing dataset samples.\n",
      "\n",
      "Input IDs batch shape: torch.Size([8, 512])\n",
      "Attention Mask batch shape: torch.Size([8, 512])\n",
      "Labels batch shape: torch.Size([8])\n",
      "\n",
      "Original text (first sample, truncated to 100 chars): for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem imagin...\n",
      "Decoded text (first sample, first 10 tokens): [CLS] dietrich bonhoeffers writings have had a\n"
     ]
    }
   ],
   "source": [
    "# 4. 文本数据加载与预处理 (使用真实的 IMDB 数据集，手动加载和清洗)\n",
    "\n",
    "# 定义 IMDB 数据集路径\n",
    "# ***重要：请确保你已经手动下载并解压 IMDB 数据集到这个路径下***\n",
    "# 下载链接：http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# 解压后，你的 data/ 目录下应该有 aclImdb/ 文件夹\n",
    "IMDB_ROOT_DIR = '../data/aclImdb' # 确保你的 aclImdb 文件夹在这个路径下\n",
    "\n",
    "# 加载真实的 IMDB 训练数据\n",
    "print(\"Loading real IMDB training data...\")\n",
    "# train/pos 和 train/neg 才是实际数据\n",
    "raw_train_data = load_imdb_data(os.path.join(IMDB_ROOT_DIR, 'train')) \n",
    "print(\"Cleaning training data (this may take a moment)...\")\n",
    "cleaned_train_data = []\n",
    "for label, text in raw_train_data:\n",
    "    cleaned_train_data.append((label, clean_text(text)))\n",
    "\n",
    "# 加载真实的 IMDB 测试数据\n",
    "print(\"Loading real IMDB testing data...\")\n",
    "# test/pos 和 test/neg 才是实际数据\n",
    "raw_test_data = load_imdb_data(os.path.join(IMDB_ROOT_DIR, 'test'))\n",
    "print(\"Cleaning testing data (this may take a moment)...\")\n",
    "cleaned_test_data = []\n",
    "for label, text in raw_test_data:\n",
    "    cleaned_test_data.append((label, clean_text(text)))\n",
    "\n",
    "# 打印加载的数据量，确认是否正确\n",
    "print(f\"Loaded {len(cleaned_train_data)} training samples.\")\n",
    "print(f\"Loaded {len(cleaned_test_data)} testing samples.\")\n",
    "print(f\"Example cleaned positive review (first 100 chars): {cleaned_train_data[0][1][:100]}...\")\n",
    "print(f\"Example cleaned negative review (first 100 chars): {cleaned_train_data[len(cleaned_train_data)//2][1][:100]}...\") # 打印一个负面评论示例\n",
    "\n",
    "# 定义自定义 Dataset 类，用于将加载的 (label, text) 对转换为 BERT 输入\n",
    "class IMDBDatasetWithBERT(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len): # 接收已加载和清洗后的数据\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.texts = [item[1] for item in data]\n",
    "        self.labels = [item[0] for item in data] # 标签已经是0或1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "# 使用 BERT 分词器进行编码\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length', \n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    # 返回模型输入格式\n",
    "# 加载预训练的 BERT tokenizer，12 层 Transformer，768 hidden size，12 attention heads；不区分大小写\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 定义最大序列长度 (BERT 通常限制为 512)\n",
    "MAX_LEN = 512 # 对于电影评论，256 可能更合理，确保能捕捉更多信息。可根据内存调整\n",
    "\n",
    "# 创建数据集实例 (使用清洗后的真实数据)\n",
    "train_dataset = IMDBDatasetWithBERT(cleaned_train_data, tokenizer, MAX_LEN)\n",
    "test_dataset = IMDBDatasetWithBERT(cleaned_test_data, tokenizer, MAX_LEN)\n",
    "\n",
    "print(f\"\\nCreated {len(train_dataset)} training dataset samples.\")\n",
    "print(f\"Created {len(test_dataset)} testing dataset samples.\")\n",
    "\n",
    "# 创建 DataLoader\n",
    "batch_size = 8 # BERT 通常使用 16 或 32 的批次大小，因为模型较大\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# 示例：获取一个批次数据\n",
    "for batch in train_dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "\n",
    "    print(f\"\\nInput IDs batch shape: {input_ids.shape}\")\n",
    "    print(f\"Attention Mask batch shape: {attention_mask.shape}\")\n",
    "    print(f\"Labels batch shape: {labels.shape}\")\n",
    "\n",
    "    # 打印第一个样本的解码文本 (只解码前一部分，因为MAX_LEN可能很长)\n",
    "    print(f\"\\nOriginal text (first sample, truncated to 100 chars): {cleaned_train_data[batch['labels'][0]][1][:100]}...\") # 对应原始加载的文本\n",
    "    print(f\"Decoded text (first sample, first 10 tokens): {tokenizer.decode(input_ids[0, :10])}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### -------------------------------------------------------------\n",
    "### 阶段三：构建和微调 Transformer 模型 (BertForSequenceClassification)\n",
    "### -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/miniconda3/envs/pytorch_env_1/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Model Structure (truncated for brevity):\n",
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 1. 模型实例化\n",
    "# num_labels 应该与你的分类任务的类别数匹配 (IMDB是二分类: 正面/负面)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"\\nBERT Model Structure (truncated for brevity):\")\n",
    "# 由于BERT模型结构非常复杂，这里只打印其顶部几层\n",
    "print(model.bert.embeddings)\n",
    "print(model.classifier)\n",
    "\n",
    "\n",
    "# 2. 定义优化器和学习率调度器\n",
    "# AdamW 是针对 Transformer 模型推荐的优化器，它修正了 Adam 中的权重衰减\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5) # 预训练模型微调的推荐学习率通常在 1e-5 到 5e-5 之间\n",
    "\n",
    "# 学习率调度器 (Hugging Face 提供了专为 Transformer 设计的调度器)\n",
    "# Get the number of training steps\n",
    "total_steps = len(train_dataloader) * 3 # 假设我们先训练 3 个 Epoch\n",
    "warmup_steps = int(0.06 * total_steps) # 可以设置一个预热步数，让学习率从0开始逐渐上升\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\", # 或 \"cosine\"\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------\n",
    "### 阶段四：训练循环 (需要适配 Hugging Face 模型输出)\n",
    "### -------------------------------------------------------------\n",
    "\n",
    "### 这里我们将直接在 Notebook 中编写训练和评估循环，因为需要适配 Hugging Face 模型的特殊输出。\n",
    "### 以后可以考虑将其封装到 train_eval_utils.py 中，但现在直接写更清晰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting BERT Fine-tuning ---\n",
      "  Epoch 1, Batch 0/3125, Loss: 0.0033\n",
      "  Epoch 1, Batch 100/3125, Loss: 0.0012\n",
      "  Epoch 1, Batch 200/3125, Loss: 0.0037\n",
      "  Epoch 1, Batch 300/3125, Loss: 0.0021\n",
      "  Epoch 1, Batch 400/3125, Loss: 0.0016\n",
      "  Epoch 1, Batch 500/3125, Loss: 0.0017\n",
      "  Epoch 1, Batch 600/3125, Loss: 0.0058\n",
      "  Epoch 1, Batch 700/3125, Loss: 0.0038\n",
      "  Epoch 1, Batch 800/3125, Loss: 0.0107\n",
      "  Epoch 1, Batch 900/3125, Loss: 0.0063\n",
      "  Epoch 1, Batch 1000/3125, Loss: 0.0009\n",
      "  Epoch 1, Batch 1100/3125, Loss: 0.0019\n",
      "  Epoch 1, Batch 1200/3125, Loss: 0.0124\n",
      "  Epoch 1, Batch 1300/3125, Loss: 0.0275\n",
      "  Epoch 1, Batch 1400/3125, Loss: 0.0039\n",
      "  Epoch 1, Batch 1500/3125, Loss: 0.0065\n",
      "  Epoch 1, Batch 1600/3125, Loss: 0.0012\n",
      "  Epoch 1, Batch 1700/3125, Loss: 0.0016\n",
      "  Epoch 1, Batch 1800/3125, Loss: 0.0012\n",
      "  Epoch 1, Batch 1900/3125, Loss: 0.0014\n",
      "  Epoch 1, Batch 2000/3125, Loss: 0.0019\n",
      "  Epoch 1, Batch 2100/3125, Loss: 0.0145\n",
      "  Epoch 1, Batch 2200/3125, Loss: 0.0013\n",
      "  Epoch 1, Batch 2300/3125, Loss: 0.0016\n",
      "  Epoch 1, Batch 2400/3125, Loss: 0.0125\n",
      "  Epoch 1, Batch 2500/3125, Loss: 0.0014\n",
      "  Epoch 1, Batch 2600/3125, Loss: 0.0023\n",
      "  Epoch 1, Batch 2700/3125, Loss: 0.0026\n",
      "  Epoch 1, Batch 2800/3125, Loss: 0.0020\n",
      "  Epoch 1, Batch 2900/3125, Loss: 0.0039\n",
      "  Epoch 1, Batch 3000/3125, Loss: 0.0049\n",
      "  Epoch 1, Batch 3100/3125, Loss: 0.0034\n",
      "Epoch 1 completed. Avg Train Loss: 0.0182, Train Acc: 99.64%\n",
      "  Epoch 2, Batch 0/3125, Loss: 0.0018\n",
      "  Epoch 2, Batch 100/3125, Loss: 0.0025\n",
      "  Epoch 2, Batch 200/3125, Loss: 0.0043\n",
      "  Epoch 2, Batch 300/3125, Loss: 0.0020\n",
      "  Epoch 2, Batch 400/3125, Loss: 0.0009\n",
      "  Epoch 2, Batch 500/3125, Loss: 0.0015\n",
      "  Epoch 2, Batch 600/3125, Loss: 0.0062\n",
      "  Epoch 2, Batch 700/3125, Loss: 0.0021\n",
      "  Epoch 2, Batch 800/3125, Loss: 0.0062\n",
      "  Epoch 2, Batch 900/3125, Loss: 0.0064\n",
      "  Epoch 2, Batch 1000/3125, Loss: 0.0015\n",
      "  Epoch 2, Batch 1100/3125, Loss: 0.0106\n",
      "  Epoch 2, Batch 1200/3125, Loss: 0.0014\n",
      "  Epoch 2, Batch 1300/3125, Loss: 0.0025\n",
      "  Epoch 2, Batch 1400/3125, Loss: 0.0036\n",
      "  Epoch 2, Batch 1500/3125, Loss: 0.0041\n",
      "  Epoch 2, Batch 1600/3125, Loss: 0.0019\n",
      "  Epoch 2, Batch 1700/3125, Loss: 0.5301\n",
      "  Epoch 2, Batch 1800/3125, Loss: 0.0016\n",
      "  Epoch 2, Batch 1900/3125, Loss: 0.0023\n",
      "  Epoch 2, Batch 2000/3125, Loss: 0.0026\n",
      "  Epoch 2, Batch 2100/3125, Loss: 0.0018\n",
      "  Epoch 2, Batch 2200/3125, Loss: 0.0096\n",
      "  Epoch 2, Batch 2300/3125, Loss: 0.0064\n",
      "  Epoch 2, Batch 2400/3125, Loss: 0.0065\n",
      "  Epoch 2, Batch 2500/3125, Loss: 0.0043\n",
      "  Epoch 2, Batch 2600/3125, Loss: 0.0045\n",
      "  Epoch 2, Batch 2700/3125, Loss: 0.0013\n",
      "  Epoch 2, Batch 2800/3125, Loss: 0.0035\n",
      "  Epoch 2, Batch 2900/3125, Loss: 0.0054\n",
      "  Epoch 2, Batch 3000/3125, Loss: 0.0034\n",
      "  Epoch 2, Batch 3100/3125, Loss: 0.0010\n",
      "Epoch 2 completed. Avg Train Loss: 0.0174, Train Acc: 99.63%\n",
      "  Epoch 3, Batch 0/3125, Loss: 0.0056\n",
      "  Epoch 3, Batch 100/3125, Loss: 0.0014\n",
      "  Epoch 3, Batch 200/3125, Loss: 0.0024\n",
      "  Epoch 3, Batch 300/3125, Loss: 0.0044\n",
      "  Epoch 3, Batch 400/3125, Loss: 0.0019\n",
      "  Epoch 3, Batch 500/3125, Loss: 0.0019\n",
      "  Epoch 3, Batch 600/3125, Loss: 0.0013\n",
      "  Epoch 3, Batch 700/3125, Loss: 0.0056\n",
      "  Epoch 3, Batch 800/3125, Loss: 0.0019\n",
      "  Epoch 3, Batch 900/3125, Loss: 0.0031\n",
      "  Epoch 3, Batch 1000/3125, Loss: 0.0070\n",
      "  Epoch 3, Batch 1100/3125, Loss: 0.1355\n",
      "  Epoch 3, Batch 1200/3125, Loss: 0.0018\n",
      "  Epoch 3, Batch 1300/3125, Loss: 0.0022\n",
      "  Epoch 3, Batch 1400/3125, Loss: 0.6071\n",
      "  Epoch 3, Batch 1500/3125, Loss: 0.0012\n",
      "  Epoch 3, Batch 1600/3125, Loss: 0.0016\n",
      "  Epoch 3, Batch 1700/3125, Loss: 0.0054\n",
      "  Epoch 3, Batch 1800/3125, Loss: 0.0020\n",
      "  Epoch 3, Batch 1900/3125, Loss: 0.1104\n",
      "  Epoch 3, Batch 2000/3125, Loss: 0.6890\n",
      "  Epoch 3, Batch 2100/3125, Loss: 0.0014\n",
      "  Epoch 3, Batch 2200/3125, Loss: 0.0026\n",
      "  Epoch 3, Batch 2300/3125, Loss: 0.0774\n",
      "  Epoch 3, Batch 2400/3125, Loss: 0.0021\n",
      "  Epoch 3, Batch 2500/3125, Loss: 0.0030\n",
      "  Epoch 3, Batch 2600/3125, Loss: 0.0029\n",
      "  Epoch 3, Batch 2700/3125, Loss: 0.0334\n",
      "  Epoch 3, Batch 2800/3125, Loss: 0.0018\n",
      "  Epoch 3, Batch 2900/3125, Loss: 0.0065\n",
      "  Epoch 3, Batch 3000/3125, Loss: 0.0062\n",
      "  Epoch 3, Batch 3100/3125, Loss: 0.0013\n",
      "Epoch 3 completed. Avg Train Loss: 0.0172, Train Acc: 99.65%\n",
      "  Epoch 4, Batch 0/3125, Loss: 0.0015\n",
      "  Epoch 4, Batch 100/3125, Loss: 0.0015\n",
      "  Epoch 4, Batch 200/3125, Loss: 0.0040\n",
      "  Epoch 4, Batch 300/3125, Loss: 0.0032\n",
      "  Epoch 4, Batch 400/3125, Loss: 0.0065\n",
      "  Epoch 4, Batch 500/3125, Loss: 0.1268\n",
      "  Epoch 4, Batch 600/3125, Loss: 0.0015\n",
      "  Epoch 4, Batch 700/3125, Loss: 0.0013\n",
      "  Epoch 4, Batch 800/3125, Loss: 0.0033\n",
      "  Epoch 4, Batch 900/3125, Loss: 0.0022\n",
      "  Epoch 4, Batch 1000/3125, Loss: 0.0037\n",
      "  Epoch 4, Batch 1100/3125, Loss: 0.0064\n",
      "  Epoch 4, Batch 1200/3125, Loss: 0.0013\n",
      "  Epoch 4, Batch 1300/3125, Loss: 0.0069\n",
      "  Epoch 4, Batch 1400/3125, Loss: 0.0109\n",
      "  Epoch 4, Batch 1500/3125, Loss: 0.0048\n",
      "  Epoch 4, Batch 1600/3125, Loss: 0.0074\n",
      "  Epoch 4, Batch 1700/3125, Loss: 0.0014\n",
      "  Epoch 4, Batch 1800/3125, Loss: 0.0020\n",
      "  Epoch 4, Batch 1900/3125, Loss: 0.0021\n",
      "  Epoch 4, Batch 2000/3125, Loss: 0.0097\n",
      "  Epoch 4, Batch 2100/3125, Loss: 0.0040\n",
      "  Epoch 4, Batch 2200/3125, Loss: 0.0012\n",
      "  Epoch 4, Batch 2300/3125, Loss: 0.0052\n",
      "  Epoch 4, Batch 2400/3125, Loss: 0.0093\n",
      "  Epoch 4, Batch 2500/3125, Loss: 0.0017\n",
      "  Epoch 4, Batch 2600/3125, Loss: 0.0063\n",
      "  Epoch 4, Batch 2700/3125, Loss: 0.0052\n",
      "  Epoch 4, Batch 2800/3125, Loss: 0.0043\n",
      "  Epoch 4, Batch 2900/3125, Loss: 0.0021\n",
      "  Epoch 4, Batch 3000/3125, Loss: 0.0018\n",
      "  Epoch 4, Batch 3100/3125, Loss: 0.0023\n",
      "Epoch 4 completed. Avg Train Loss: 0.0171, Train Acc: 99.66%\n",
      "--- BERT Fine-tuning Finished ---\n"
     ]
    }
   ],
   "source": [
    "num_epochs_bert = 4 # 预训练模型通常 2-4 个 Epoch 就能达到好效果\n",
    "\n",
    "print(\"\\n--- Starting BERT Fine-tuning ---\")\n",
    "\n",
    "for epoch in range(num_epochs_bert):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Hugging Face 模型的前向传播\n",
    "        # 返回一个字典，其中包含 loss 和 logits\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits # Logits 用于计算准确率\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step() # 学习率调度器更新\n",
    "\n",
    "        # 计算训练准确率\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        if batch_idx % 100 == 0: # 每 100 个批次打印一次进度\n",
    "            print(f\"  Epoch {epoch+1}, Batch {batch_idx}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions / total_samples * 100\n",
    "    print(f\"Epoch {epoch+1} completed. Avg Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "print(\"--- BERT Fine-tuning Finished ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------\n",
    "### 阶段五：模型评估\n",
    "### -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting BERT Evaluation with Comprehensive Metrics ---\n",
      "Evaluation completed. Avg Test Loss: 0.2259\n",
      "Accuracy: 0.9380\n",
      "Precision: 0.9389\n",
      "Recall: 0.9370\n",
      "F1-Score: 0.9380\n",
      "--- BERT Evaluation Finished ---\n",
      "\n",
      "Confusion Matrix (DataFrame):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns # 用于绘制混淆矩阵\n",
    "import pandas as pd # 用于混淆矩阵的DataFrame\n",
    "\n",
    "print(\"\\n--- Starting BERT Evaluation with Comprehensive Metrics ---\")\n",
    "model.eval() \n",
    "all_labels = []\n",
    "all_preds = []\n",
    "total_eval_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "avg_eval_loss = total_eval_loss / len(test_dataloader)\n",
    "\n",
    "# 计算各项指标\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "# precision_recall_fscore_support 默认会为每个类别计算，这里需要指定 average 参数\n",
    "# binary: 适用于二分类\n",
    "# macro: 简单平均每个类别的指标\n",
    "# weighted: 按每个类别样本数加权平均\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', pos_label=1) # positive label is 1\n",
    "\n",
    "print(f\"Evaluation completed. Avg Test Loss: {avg_eval_loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"--- BERT Evaluation Finished ---\")\n",
    "\n",
    "# 绘制混淆矩阵\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_df = pd.DataFrame(cm, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive'])\n",
    "# 打印 DataFrame 格式\n",
    "print(\"\\nConfusion Matrix (DataFrame):\")\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### -------------------------------------------------------------\n",
    "### 阶段六：模型保存与加载 (适配 Hugging Face 模型)\n",
    "### -------------------------------------------------------------\n",
    "\n",
    "### Hugging Face 模型的保存和加载与 torch.save 略有不同\n",
    "### 推荐使用其自带的 save_pretrained 和 from_pretrained 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to ../model_checkpoints/bert_imdb_sentiment\n",
      "Model saved.\n",
      "\n",
      "Loading model from ../model_checkpoints/bert_imdb_sentiment\n",
      "Loaded Model Evaluation. Avg Test Loss: 0.2259\n",
      "Accuracy: 0.9380\n",
      "Precision: 0.9389\n",
      "Recall: 0.9370\n",
      "F1-Score: 0.9380\n"
     ]
    }
   ],
   "source": [
    "output_dir = '../model_checkpoints/bert_imdb_sentiment'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nSaving model to {output_dir}\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir) # 也要保存分词器，因为它的词汇表和模型是配套的\n",
    "print(\"Model saved.\")\n",
    "\n",
    "# 加载模型示例\n",
    "print(f\"\\nLoading model from {output_dir}\")\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "# 再次评估加载后的模型\n",
    "loaded_all_labels = []\n",
    "loaded_all_preds = []\n",
    "loaded_total_eval_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loaded_total_eval_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        \n",
    "        loaded_all_labels.extend(labels.cpu().numpy())\n",
    "        loaded_all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "loaded_avg_eval_loss = loaded_total_eval_loss / len(test_dataloader)\n",
    "loaded_accuracy = accuracy_score(loaded_all_labels, loaded_all_preds)\n",
    "loaded_precision, loaded_recall, loaded_f1, _ = precision_recall_fscore_support(loaded_all_labels, loaded_all_preds, average='binary', pos_label=1)\n",
    "\n",
    "print(f\"Loaded Model Evaluation. Avg Test Loss: {loaded_avg_eval_loss:.4f}\")\n",
    "print(f\"Accuracy: {loaded_accuracy:.4f}\")\n",
    "print(f\"Precision: {loaded_precision:.4f}\")\n",
    "print(f\"Recall: {loaded_recall:.4f}\")\n",
    "print(f\"F1-Score: {loaded_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------\n",
    "### 阶段七：模型推理与应用\n",
    "### -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review: 'This movie was absolutely brilliant! A true masterpiece with outstanding performances and a captivating story. Highly recommended.'\n",
      "Predicted Sentiment: Positive (Positive Probability: 0.9991)\n",
      "\n",
      "Review: 'What a terrible film. The plot made no sense, the acting was wooden, and I walked out halfway through. A complete waste of time and money.'\n",
      "Predicted Sentiment: Negative (Positive Probability: 0.0006)\n",
      "\n",
      "Review: 'The movie had some good parts, but it was also quite slow in others. Overall, it was just okay, nothing special.'\n",
      "Predicted Sentiment: Negative (Positive Probability: 0.0109)\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, max_len=256):\n",
    "    \"\"\"\n",
    "    预测单条电影评论的情感。\n",
    "\n",
    "    Args:\n",
    "        text (str): 待预测的电影评论文本。\n",
    "        model (BertForSequenceClassification): 训练好的 BERT 模型。\n",
    "        tokenizer (BertTokenizer): 对应的 BERT 分词器。\n",
    "        device (torch.device): 模型所在的设备 (CPU/MPS)。\n",
    "        max_len (int): 最大序列长度，需与训练时保持一致。\n",
    "\n",
    "    Returns:\n",
    "        str: 预测的情感类别 ('Positive' 或 'Negative')。\n",
    "        float: 预测为正例的概率。\n",
    "    \"\"\"\n",
    "    model.eval() # 设置模型为评估模式\n",
    "\n",
    "    # 1. 文本预处理\n",
    "    cleaned_text = clean_text(text) # 使用之前定义的清洗函数\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        cleaned_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt', # 返回 PyTorch 张量\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # 2. 将输入移动到设备\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    # 3. 模型推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits # 获取模型的原始输出 (logits)\n",
    "\n",
    "    # 4. 转换为概率和预测类别\n",
    "    # 对于二分类，logits通常是一个包含两个值的张量 [logit_negative, logit_positive]\n",
    "    # 或者直接是标量，表示正例的logit\n",
    "    # 使用 sigmoid 将 logits 转换为概率 (因为是二分类，CrossEntropyLoss 内部做了softmax，但这里我们想要单类概率)\n",
    "    # 对于 BertForSequenceClassification，输出是 logits，所以需要 softmax 来获得概率\n",
    "    probs = torch.softmax(logits, dim=1) # dim=1 表示在类别维度上进行softmax\n",
    "    \n",
    "    # 预测为正例的概率\n",
    "    positive_probability = probs[:, 1].item() # 获取索引1（正例）的概率\n",
    "\n",
    "    # 预测类别\n",
    "    predicted_class_id = torch.argmax(probs, dim=1).item() # 获取概率最大的类别索引\n",
    "    \n",
    "    sentiment = \"Positive\" if predicted_class_id == 1 else \"Negative\"\n",
    "\n",
    "    return sentiment, positive_probability\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 示例使用模型进行推理 (加载模型后运行此部分)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# 假设你的模型已经加载到 loaded_model 和 loaded_tokenizer 中\n",
    "# 如果还没有运行模型加载部分，请先运行\n",
    "if 'loaded_model' not in locals() or 'loaded_tokenizer' not in locals():\n",
    "    print(\"Loading model for inference...\")\n",
    "    output_dir = '../model_checkpoints/bert_imdb_sentiment'\n",
    "    loaded_model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "    loaded_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "    loaded_model = loaded_model.to(device)\n",
    "    loaded_model.eval()\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "# 测试评论 1：积极评论\n",
    "review1 = \"This movie was absolutely brilliant! A true masterpiece with outstanding performances and a captivating story. Highly recommended.\"\n",
    "sentiment1, prob1 = predict_sentiment(review1, loaded_model, loaded_tokenizer, device, MAX_LEN)\n",
    "print(f\"\\nReview: '{review1}'\")\n",
    "print(f\"Predicted Sentiment: {sentiment1} (Positive Probability: {prob1:.4f})\")\n",
    "\n",
    "# 测试评论 2：消极评论\n",
    "review2 = \"What a terrible film. The plot made no sense, the acting was wooden, and I walked out halfway through. A complete waste of time and money.\"\n",
    "sentiment2, prob2 = predict_sentiment(review2, loaded_model, loaded_tokenizer, device, MAX_LEN)\n",
    "print(f\"\\nReview: '{review2}'\")\n",
    "print(f\"Predicted Sentiment: {sentiment2} (Positive Probability: {prob2:.4f})\")\n",
    "\n",
    "# 测试评论 3：中性或模糊评论 (看模型如何处理)\n",
    "review3 = \"The movie had some good parts, but it was also quite slow in others. Overall, it was just okay, nothing special.\"\n",
    "sentiment3, prob3 = predict_sentiment(review3, loaded_model, loaded_tokenizer, device, MAX_LEN)\n",
    "print(f\"\\nReview: '{review3}'\")\n",
    "print(f\"Predicted Sentiment: {sentiment3} (Positive Probability: {prob3:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
